# -*- coding: utf-8 -*-
"""Prova

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JxmtuAu95kyXsQqAyRfflVpsyI_5UqHW
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/2025/Sistemas Inteligentes Avançados/Models/diabetic_data.csv', sep=',')

from collections import Counter
import numpy as np

# Primeiro, substituir os '?' por NaN
df.replace('?', np.nan, inplace=True)
df.drop(columns=['diag_1', 'diag_2', 'diag_3'], inplace=True, errors='ignore')

# Definir um limite de tolerância para "muitos" NaNs
# Exemplo: se mais de 30% dos valores estão faltando, dropa
limite_na = 0.5  # 30%


# Dropar colunas com muitos valores NaN
colunas_para_dropar = df.columns[df.isna().mean() > limite_na]
df.drop(columns=colunas_para_dropar, inplace=True)

# Agora, preencher os NaNs restantes
for coluna in df.columns:
    if df[coluna].isna().sum() > 0:
        if df[coluna].dtype == 'object':
            # Preencher categóricas com a moda
            moda = df[coluna].mode()[0]
            df[coluna].fillna(moda, inplace=True)
        else:
            # Preencher numéricas com a média
            media = df[coluna].astype(int).mean()
            df[coluna].fillna(media, inplace=True)

# Encontrar colunas onde um único valor domina
colunas_generalizantes = []

for coluna in df.columns:
    valor_mais_frequente = df[coluna].value_counts(normalize=True).iloc[0]
    if valor_mais_frequente > 0.95:
        colunas_generalizantes.append(coluna)

# Remover essas colunas
df.drop(columns=colunas_generalizantes, inplace=True)

print(f'Colunas generalizantes removidas: {colunas_generalizantes}')
print(f'Número de colunas restantes: {df.shape[1]}')

# Identificar colunas categóricas
colunas_categoricas = df.select_dtypes(include='object').columns


# Aplicar One-Hot Encoding
df = pd.get_dummies(df, columns=colunas_categoricas)
df.shape

from sklearn.preprocessing import MinMaxScaler #normalizar

scaler = MinMaxScaler()
dfNormalizada = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

from sklearn.cluster import KMeans
import math
from scipy.spatial.distance import cdist #Método de determinação de distancias
import numpy as np

# pegando amostra
amostra = dfNormalizada.sample(frac=0.01)
amostra.shape

numAmostras = len(amostra)

distortion = []
K= range(1, numAmostras)

for k in K:
    clusterModel = KMeans(n_clusters=k).fit(amostra)

    # calcular distorção
    distortion.append(
        sum(
            np.min(
                cdist(amostra, clusterModel.cluster_centers_, 'euclidean'), axis=1
            )/amostra.shape[0]
        )
    )
distortion

import math
import numpy as np

x0 = K[0]
y0 = distortion[0]
xn = K[-1]
yn = distortion[-1]
distancias = []

for i in range(len(distortion)):
    x = K[i]
    y = distortion[i]
    numerador = abs((y - y0) * (xn - x0) - (x - x0) * (yn - y0))
    denominador = math.hypot(xn - x0, yn - y0)  # melhor para evitar divisão por zero

    if denominador == 0:
        distancia = 0
    else:
        distancia = numerador / denominador

    distancias.append(distancia)

# Agora encontrar o número ótimo de clusters
numOtimoClusters = K[np.argmax(distancias)]

numOtimoClusters

#Treinar

DiabeteClusterModel = KMeans(n_clusters = numOtimoClusters).fit(df)

# Desnormalizar

dfNormalizada['cluster'] = DiabeteClusterModel.labels_

dfDesnormalizado = pd.DataFrame(scaler.inverse_transform(dfNormalizada.drop('cluster', axis=1)), columns=dfNormalizada.drop('cluster', axis=1).columns)
dfDesnormalizado['cluster'] = dfNormalizada['cluster']
dfDesnormalizado.head()

def inferir_cluster(nova_instancia, scaler, kmeans, dfNormalizada, dfDesnormalizado):
    # 1. Converte a nova instância para DataFrame
    nova_df = pd.DataFrame([nova_instancia])

    # 2. Garante que tenha todas as colunas na mesma ordem
    nova_df = nova_df.reindex(columns=dfNormalizada.drop(columns='cluster').columns, fill_value=0)

    # 3. Normaliza a nova instância
    nova_normalizada = scaler.transform(nova_df)

    # 4. Prediz o cluster
    cluster_predito = kmeans.predict(nova_normalizada)[0]

    print(f"\nA nova instância pertence ao cluster: {cluster_predito}\n")

    # 5. Pega os dados desnormalizados correspondentes
    dados_cluster = dfDesnormalizado[dfDesnormalizado['cluster'] == cluster_predito]

    print("Média dos dados neste cluster (desnormalizados):")
    print(dados_cluster.drop(columns='cluster').mean().round(2))

    print("\nExemplo real desse cluster:")
    print(dados_cluster.drop(columns='cluster').sample(1).T)

#  1. Criar uma nova instância preenchida corretamente
nova_instancia = {
    coluna: 1 if 'gender_' in coluna or 'age_' in coluna or 'race_' in coluna or 'admission_' in coluna else 5
    for coluna in dfNormalizada.drop('cluster', axis=1).columns
}

# 2. Inferir o cluster da nova instância
inferir_cluster(nova_instancia, scaler, DiabeteClusterModel, dfNormalizada, dfDesnormalizado)